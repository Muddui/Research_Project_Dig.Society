{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKFLOW\n",
    "\n",
    "Before coding: Fetch latest origin in GitHub Desktop.\n",
    "\n",
    "Work using your own separate notebooks. Transfer code to MAIN.ipynb once your own file works.\n",
    "\n",
    "After coding: Copy code to this MAIN.ipynb -> Summary of changes -> Commit to main -> Push Origin.\n",
    "\n",
    "Go to https://github.com/ndquoctrong/CCS2 to double check.\n",
    "\n",
    "# GAMEPLAN\n",
    "\n",
    "Produce a hybrid recommender system that takes user input innitially (collaborative filtering) to narrow down the dataset, then topic model the description of the remaining books to give recommendations (content-based)\n",
    "\n",
    "## User input\n",
    "\n",
    "All English -> language dont matter\n",
    "\n",
    "Page Number -> maximum? range?\n",
    "\n",
    "Genre Filter\n",
    "\n",
    "Author Filter\n",
    "\n",
    "Choose to display results by ranking or popularity (number of voters)\n",
    "\n",
    "implement a minimum voter count to count the rankings ()\n",
    "\n",
    "## Topic modelling\n",
    "\n",
    "Descriptions\n",
    "\n",
    "Genre column\n",
    "\n",
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version: 4.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mucki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import stopwords\n",
    "import requests\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "\n",
    "from glob import glob\n",
    "from dateutil import parser\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "print(f\"gensim version: {gensim.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def df_expl (df):\n",
    "    desc = {\"Columns\": df.columns, \"Missing\": df.isna().sum(), \"D_Types\": df.dtypes, \"Shape\": df.shape}\n",
    "    return desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mucki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version: 4.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import stopwords\n",
    "import requests\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "\n",
    "from glob import glob\n",
    "from dateutil import parser\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "print(f\"gensim version: {gensim.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def df_expl (df):\n",
    "    desc = {\"Columns\": df.columns, \"Missing\": df.isna().sum(), \"D_Types\": df.dtypes, \"Shape\": df.shape}\n",
    "    return desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to import the original dataset in csv. Note that it will trigger heavy Google API use. More infor later at *\"Fill in missing values from Google Books API\"* section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'books_1299 = pd.read_csv(\"google_books_1299.csv\", sep = \",\")\\nbooks_1299[\\'voters\\'] = books_1299[\\'voters\\'].str.replace(\\',\\', \\'\\')'"
      ]
     },
     "execution_count": 1281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''books_1299 = pd.read_csv(\"google_books_1299.csv\", sep = \",\")\n",
    "books_1299['voters'] = books_1299['voters'].str.replace(',', '')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this following block instead to import an excel file with Google API results pre-loaded. Also saves time besides avoiding API response limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv=pd.read_csv(\"google_books_1299 OG.csv\", sep = \",\") # save the original csv as backup\n",
    "csv.to_excel(\"booksOG.xlsx\")\n",
    "books_1299 = pd.read_excel(\"booksresults.xlsx\")\n",
    "#books_data = pd.read_csv(\"google_books_dataset.csv\", sep = \",\")\n",
    "\n",
    "#books_1299.to_excel('booksOG.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'title', 'author', 'rating', 'voters', 'price',\n",
       "       'currency', 'description', 'publisher', 'page_count', 'generes', 'ISBN',\n",
       "       'language', 'published_date', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_1299.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                 int64\n",
       "title                     object\n",
       "author                    object\n",
       "rating                   float64\n",
       "voters                   float64\n",
       "price                    float64\n",
       "currency                  object\n",
       "description               object\n",
       "publisher                 object\n",
       "page_count                 int64\n",
       "generes                   object\n",
       "ISBN                      object\n",
       "language                  object\n",
       "published_date    datetime64[ns]\n",
       "year                     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_1299.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_1299.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "English    1299\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_1299['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         0\n",
       "title              0\n",
       "author             0\n",
       "rating            75\n",
       "voters            75\n",
       "price              0\n",
       "currency           0\n",
       "description        3\n",
       "publisher          0\n",
       "page_count         0\n",
       "generes           67\n",
       "ISBN               0\n",
       "language           0\n",
       "published_date     2\n",
       "year               2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_1299.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_1299['rating'] = books_1299['rating'].astype(float).round(1)\n",
    "books_1299['voters'] = books_1299['voters'].astype(float).round(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up time data\n",
    "\n",
    "(see *build_a_recommender.ipynb* for reference)\n",
    "\n",
    "Take only year data, no need for dates and months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2014\n",
      "1       2007\n",
      "2       2019\n",
      "3       2009\n",
      "4       2009\n",
      "        ... \n",
      "1294    2019\n",
      "1295    2017\n",
      "1296    2010\n",
      "1297    2014\n",
      "1298    2013\n",
      "Name: year, Length: 1299, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "#convert the published date into standard format\n",
    "books_1299['published_date'] = pd.to_datetime(books_1299['published_date'], format='%b %d, %Y',errors='coerce')\n",
    "#extract the release year into a separate columnm, in integer for later analysis\n",
    "books_1299['year'] = books_1299['published_date'].dt.year.astype('Int64')\n",
    "# print the result\n",
    "print(books_1299['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the 'generes' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"books_1299['voters'] = books_1299['voters'].fillna(0)\\nbooks_1299['voters'] = books_1299['voters'].str.replace(',', '').astype(int)\\nprint(books_1299['voters'].describe(percentiles=[0.25, 0.5, 0.75]))\""
      ]
     },
     "execution_count": 1289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''books_1299['voters'] = books_1299['voters'].fillna(0)\n",
    "books_1299['voters'] = books_1299['voters'].str.replace(',', '').astype(int)\n",
    "print(books_1299['voters'].describe(percentiles=[0.25, 0.5, 0.75]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'generes' column to string of genres separated by comma\n",
    "books_1299['generes'] = books_1299['generes'].astype(str)\n",
    "books_1299['generes'] = books_1299['generes'].str.replace(',amp', '').replace('&amp', '').replace(' &', ',')\n",
    "for i in range(len(books_1299)):\n",
    "    genres_list = books_1299.loc[i, 'generes']\n",
    "    if isinstance(genres_list, str):\n",
    "        genres_list = genres_list.split(',')\n",
    "        genres_list = [genre.lower().strip() for genre in genres_list]\n",
    "        genres_str = ','.join(genres_list)\n",
    "        books_1299.at[i, 'generes'] = genres_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values from Google Books API\n",
    "Avoid importing the original csv more than once consecutively, as the Google Books API has a response limit (100 requests per 100 seconds per user). If over used the API Key will be temporary locked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>voters</th>\n",
       "      <th>price</th>\n",
       "      <th>currency</th>\n",
       "      <th>description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>page_count</th>\n",
       "      <th>generes</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>language</th>\n",
       "      <th>published_date</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Attack on Titan: Volume 13</td>\n",
       "      <td>Hajime Isayama</td>\n",
       "      <td>4.6</td>\n",
       "      <td>428.0</td>\n",
       "      <td>43.28</td>\n",
       "      <td>SAR</td>\n",
       "      <td>NO SAFE PLACE LEFT At great cost to the Garris...</td>\n",
       "      <td>Kodansha Comics</td>\n",
       "      <td>192</td>\n",
       "      <td>comics,graphic novels</td>\n",
       "      <td>9781612626864</td>\n",
       "      <td>English</td>\n",
       "      <td>2014-07-31</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Antiques Roadkill: A Trash 'n' Treasures Mystery</td>\n",
       "      <td>Barbara Allan</td>\n",
       "      <td>3.3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.15</td>\n",
       "      <td>SAR</td>\n",
       "      <td>Determined to make a new start in her quaint h...</td>\n",
       "      <td>Kensington Publishing Corp.</td>\n",
       "      <td>288</td>\n",
       "      <td>fiction,mystery &amp;amp,detective,cozy,general</td>\n",
       "      <td>9780758272799</td>\n",
       "      <td>English</td>\n",
       "      <td>2007-07-01</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Art of Super Mario Odyssey</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>133.85</td>\n",
       "      <td>SAR</td>\n",
       "      <td>Take a globetrotting journey all over the worl...</td>\n",
       "      <td>Dark Horse Comics</td>\n",
       "      <td>368</td>\n",
       "      <td>games &amp;amp,activities,video &amp;amp,electronic</td>\n",
       "      <td>9781506713816</td>\n",
       "      <td>English</td>\n",
       "      <td>2019-11-05</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Getting Away Is Deadly: An Ellie Avery Mystery</td>\n",
       "      <td>Sara Rosett</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.15</td>\n",
       "      <td>SAR</td>\n",
       "      <td>With swollen feet and swelling belly, pregnant...</td>\n",
       "      <td>Kensington Publishing Corp.</td>\n",
       "      <td>320</td>\n",
       "      <td>fiction</td>\n",
       "      <td>9781617734076</td>\n",
       "      <td>English</td>\n",
       "      <td>2009-03-01</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Painted Man (The Demon Cycle, Book 1)</td>\n",
       "      <td>Peter V. Brett</td>\n",
       "      <td>4.5</td>\n",
       "      <td>577.0</td>\n",
       "      <td>28.54</td>\n",
       "      <td>SAR</td>\n",
       "      <td>The stunning debut fantasy novel from author P...</td>\n",
       "      <td>HarperCollins UK</td>\n",
       "      <td>544</td>\n",
       "      <td>fiction,fantasy,dark fantasy</td>\n",
       "      <td>9780007287758</td>\n",
       "      <td>English</td>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             title   \n",
       "0           0                        Attack on Titan: Volume 13  \\\n",
       "1           1  Antiques Roadkill: A Trash 'n' Treasures Mystery   \n",
       "2           2                    The Art of Super Mario Odyssey   \n",
       "3           3    Getting Away Is Deadly: An Ellie Avery Mystery   \n",
       "4           4         The Painted Man (The Demon Cycle, Book 1)   \n",
       "\n",
       "           author  rating  voters   price currency   \n",
       "0  Hajime Isayama     4.6   428.0   43.28      SAR  \\\n",
       "1   Barbara Allan     3.3    23.0   26.15      SAR   \n",
       "2        Nintendo     3.9     9.0  133.85      SAR   \n",
       "3     Sara Rosett     4.0    10.0   26.15      SAR   \n",
       "4  Peter V. Brett     4.5   577.0   28.54      SAR   \n",
       "\n",
       "                                         description   \n",
       "0  NO SAFE PLACE LEFT At great cost to the Garris...  \\\n",
       "1  Determined to make a new start in her quaint h...   \n",
       "2  Take a globetrotting journey all over the worl...   \n",
       "3  With swollen feet and swelling belly, pregnant...   \n",
       "4  The stunning debut fantasy novel from author P...   \n",
       "\n",
       "                     publisher  page_count   \n",
       "0              Kodansha Comics         192  \\\n",
       "1  Kensington Publishing Corp.         288   \n",
       "2            Dark Horse Comics         368   \n",
       "3  Kensington Publishing Corp.         320   \n",
       "4             HarperCollins UK         544   \n",
       "\n",
       "                                       generes           ISBN language   \n",
       "0                        comics,graphic novels  9781612626864  English  \\\n",
       "1  fiction,mystery &amp,detective,cozy,general  9780758272799  English   \n",
       "2  games &amp,activities,video &amp,electronic  9781506713816  English   \n",
       "3                                      fiction  9781617734076  English   \n",
       "4                 fiction,fantasy,dark fantasy  9780007287758  English   \n",
       "\n",
       "  published_date  year  \n",
       "0     2014-07-31  2014  \n",
       "1     2007-07-01  2007  \n",
       "2     2019-11-05  2019  \n",
       "3     2009-03-01  2009  \n",
       "4     2009-01-08  2009  "
      ]
     },
     "execution_count": 1291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Provide API key to access Google Books API and retrieve the missing values\n",
    "        \n",
    "api_key = \"AIzaSyCHqQoTAYEMkQj11SFco__9BZJ_Eo0JlbA\"\n",
    "\n",
    "# Set up API parameters from documentation\n",
    "url = 'https://www.googleapis.com/books/v1/volumes'\n",
    "\n",
    "# Iterate over each row and update 'none' values in 'generes' column\n",
    "for index, row in books_1299.iterrows():\n",
    "    if 'none' in row['generes']:\n",
    "        # Make API request to Google Books API\n",
    "        book_title = row['title']\n",
    "        params = {'q': book_title, 'key': api_key}\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        # Print response code\n",
    "        print(f'Response code for {book_title}: {response.status_code}')\n",
    "        \n",
    "        # Extract genre information from API response\n",
    "        try:\n",
    "            genre_response = response.json()['items'][0]['volumeInfo']['categories']\n",
    "            genre_info = ', '.join([genre.replace(' &', ',').replace('&amp', ',').lower() for genre in genre_response])\n",
    "        except KeyError:\n",
    "            print(f'Error updating genres for book with title: {book_title}')\n",
    "            # Set 'none' genre to an empty list of strings\n",
    "            genre_info = ''\n",
    "        except ValueError:\n",
    "            print(f'Error converting genre info for book with title: {book_title}')\n",
    "            # Set 'none' genre to an empty string\n",
    "            genre_info = ''\n",
    "        \n",
    "        # Update 'genres' column with genre information\n",
    "        books_1299.at[index, 'generes'] = genre_info\n",
    "    else:\n",
    "        #print('No change')\n",
    "        pass\n",
    "\n",
    "#print results to excel for convenience of avoiding\n",
    "#  using API repeatedly, as explained in the markdowns\n",
    "\n",
    "#books_1299.to_excel('booksresults.xlsx', index=False) \n",
    "books_1299.head()\n",
    "\n",
    "# Print updated DataFrame\n",
    "#print(books_1299)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>voters</th>\n",
       "      <th>description</th>\n",
       "      <th>page_count</th>\n",
       "      <th>generes</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>published_date</th>\n",
       "      <th>year</th>\n",
       "      <th>single_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Attack on Titan: Volume 13</td>\n",
       "      <td>Hajime Isayama</td>\n",
       "      <td>4.6</td>\n",
       "      <td>428.0</td>\n",
       "      <td>NO SAFE PLACE LEFT At great cost to the Garris...</td>\n",
       "      <td>192</td>\n",
       "      <td>comics,graphic novels</td>\n",
       "      <td>9781612626864</td>\n",
       "      <td>2014-07-31</td>\n",
       "      <td>2014</td>\n",
       "      <td>comics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Attack on Titan: Volume 13</td>\n",
       "      <td>Hajime Isayama</td>\n",
       "      <td>4.6</td>\n",
       "      <td>428.0</td>\n",
       "      <td>NO SAFE PLACE LEFT At great cost to the Garris...</td>\n",
       "      <td>192</td>\n",
       "      <td>comics,graphic novels</td>\n",
       "      <td>9781612626864</td>\n",
       "      <td>2014-07-31</td>\n",
       "      <td>2014</td>\n",
       "      <td>graphic novels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Antiques Roadkill: A Trash 'n' Treasures Mystery</td>\n",
       "      <td>Barbara Allan</td>\n",
       "      <td>3.3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Determined to make a new start in her quaint h...</td>\n",
       "      <td>288</td>\n",
       "      <td>fiction,mystery &amp;amp,detective,cozy,general</td>\n",
       "      <td>9780758272799</td>\n",
       "      <td>2007-07-01</td>\n",
       "      <td>2007</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Antiques Roadkill: A Trash 'n' Treasures Mystery</td>\n",
       "      <td>Barbara Allan</td>\n",
       "      <td>3.3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Determined to make a new start in her quaint h...</td>\n",
       "      <td>288</td>\n",
       "      <td>fiction,mystery &amp;amp,detective,cozy,general</td>\n",
       "      <td>9780758272799</td>\n",
       "      <td>2007-07-01</td>\n",
       "      <td>2007</td>\n",
       "      <td>mystery &amp;amp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Antiques Roadkill: A Trash 'n' Treasures Mystery</td>\n",
       "      <td>Barbara Allan</td>\n",
       "      <td>3.3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Determined to make a new start in her quaint h...</td>\n",
       "      <td>288</td>\n",
       "      <td>fiction,mystery &amp;amp,detective,cozy,general</td>\n",
       "      <td>9780758272799</td>\n",
       "      <td>2007-07-01</td>\n",
       "      <td>2007</td>\n",
       "      <td>detective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             title   \n",
       "0           0                        Attack on Titan: Volume 13  \\\n",
       "0           0                        Attack on Titan: Volume 13   \n",
       "1           1  Antiques Roadkill: A Trash 'n' Treasures Mystery   \n",
       "1           1  Antiques Roadkill: A Trash 'n' Treasures Mystery   \n",
       "1           1  Antiques Roadkill: A Trash 'n' Treasures Mystery   \n",
       "\n",
       "           author  rating  voters   \n",
       "0  Hajime Isayama     4.6   428.0  \\\n",
       "0  Hajime Isayama     4.6   428.0   \n",
       "1   Barbara Allan     3.3    23.0   \n",
       "1   Barbara Allan     3.3    23.0   \n",
       "1   Barbara Allan     3.3    23.0   \n",
       "\n",
       "                                         description  page_count   \n",
       "0  NO SAFE PLACE LEFT At great cost to the Garris...         192  \\\n",
       "0  NO SAFE PLACE LEFT At great cost to the Garris...         192   \n",
       "1  Determined to make a new start in her quaint h...         288   \n",
       "1  Determined to make a new start in her quaint h...         288   \n",
       "1  Determined to make a new start in her quaint h...         288   \n",
       "\n",
       "                                       generes           ISBN published_date   \n",
       "0                        comics,graphic novels  9781612626864     2014-07-31  \\\n",
       "0                        comics,graphic novels  9781612626864     2014-07-31   \n",
       "1  fiction,mystery &amp,detective,cozy,general  9780758272799     2007-07-01   \n",
       "1  fiction,mystery &amp,detective,cozy,general  9780758272799     2007-07-01   \n",
       "1  fiction,mystery &amp,detective,cozy,general  9780758272799     2007-07-01   \n",
       "\n",
       "   year    single_genre  \n",
       "0  2014          comics  \n",
       "0  2014  graphic novels  \n",
       "1  2007         fiction  \n",
       "1  2007    mystery &amp  \n",
       "1  2007       detective  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split each genre into multiple single genres, each in their own row (see \"build_a_recommender\" for reference)\n",
    "s = books_1299.apply(lambda x: pd.Series(x['generes'].split(',')), axis=1).stack().reset_index(level=1, drop=True)\n",
    "s.name = 'single_genre'\n",
    "books_1299 = books_1299.join(s)\n",
    "\n",
    "# drop all unneccessary coulumns to reduce size\n",
    "books_1299.drop(['price', 'currency', 'publisher', 'language'], axis=1, inplace=True)\n",
    "\n",
    "books_1299.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "Get user to input data and filter *books_1299* accordingly\n",
    "\n",
    "Basic structure of all the main code blocks: \n",
    "\n",
    "- Prompt for user input\n",
    "\n",
    "- If or While loop to check if input is not empty. If empty, skip.\n",
    "\n",
    "- Error handling: check if format is right, if not, either return *None* or prompt for input again.\n",
    "\n",
    "- Do whatever format/edit needed, with supporting function blocks.\n",
    "\n",
    "- Return a dataframe of books *filtered_books* that match all the filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-input books\n",
    "\n",
    "Get user to input a string of ISBNs separated by commas. \n",
    "\n",
    "Check if the ISBNs are formatted correctly\n",
    "\n",
    "Find the ISBNs in the dataframe, if not available, search on Google Books API\n",
    "\n",
    "Return a *user_book* dataframe with basic info (title, author, genres, description) for analysis later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if a isbn is formatted correctly\n",
    "def isbn_check(isbn):\n",
    "    if len(isbn) == 10 or len(isbn) == 13:\n",
    "        try:\n",
    "            int(isbn)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    elif isbn.strip() == '':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve book information from Google Books API\n",
    "def get_book_info(isbn):\n",
    "    try:\n",
    "        url = f\"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}\"\n",
    "        response = requests.get(url)\n",
    "        user_data = response.json()['items'][0]['volumeInfo']\n",
    "        title = user_data['title',\"\"]\n",
    "        authors = user_data['authors',\"\"]\n",
    "        genre = user_data['categories',\"\"]\n",
    "        description = user_data['description',\"\"]\n",
    "        return title, authors, genre, description\n",
    "    #error handling\n",
    "    except KeyError:\n",
    "        print(\"Error: missing book information.\")\n",
    "        title=\"\"\n",
    "        authors=\"\"\n",
    "        genre=\"\"\n",
    "        description=\"\"\n",
    "        return title, authors, genre, description\n",
    "    except ValueError:\n",
    "        print(\"Error: invalid API response.\")\n",
    "        title=\"\"\n",
    "        authors=\"\"\n",
    "        genre=\"\"\n",
    "        description=\"\"\n",
    "        return title, authors, genre, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a comma-separated list of ISBNs from the user\n",
    "\n",
    "def get_user_books():\n",
    "    user_isbn = input(\"Enter a list of your favourite books as ISBNs separated by commas (leave blank if you don't have a favourite): \").strip()\n",
    "    if user_isbn != \"\":\n",
    "        # Split the input string into a list of ISBNs\n",
    "        user_isbn = user_isbn.split(',')\n",
    "\n",
    "        # Remove any leading/trailing whitespace from the ISBNs\n",
    "        user_isbn = [isbn.strip() for isbn in user_isbn]\n",
    "\n",
    "        # Prompt the user to enter valid ISBNs until all inputs are valid\n",
    "        while not all(isbn_check(isbn) for isbn in user_isbn):\n",
    "            print(\"One or more of the entered ISBNs is invalid. Please try again.\")\n",
    "            user_isbn = input(\"Enter a list of ISBNs separated by commas (leave blank to skip): \").strip()\n",
    "            user_isbn = user_isbn.split(',')\n",
    "            user_isbn = [isbn.strip() for isbn in user_isbn]\n",
    "\n",
    "        # Print the list of valid ISBNs\n",
    "        print(\"The following ISBNs are valid: \", user_isbn)\n",
    "        user_books=pd.DataFrame()\n",
    "        for isbn in user_isbn:\n",
    "            # Check if ISBN is in DataFrame\n",
    "            if books_1299['ISBN'].str.contains(isbn).any():\n",
    "                print(f\"{isbn} is in the DataFrame!\")\n",
    "                row = books_1299.loc[books_1299['ISBN'] == isbn]\n",
    "                #print(row)\n",
    "                #get book information from the existing dataframe\n",
    "                book_dict = {'ISBN': isbn,\n",
    "                    'title': row['title'].values[0],\n",
    "                    'author': row['author'].values[0],\n",
    "                    'generes': row['generes'].values[0],\n",
    "                    'description': row['description'].values[0]}\n",
    "                book_df = pd.DataFrame([book_dict])\n",
    "                user_books=pd.concat([user_books, book_df], ignore_index=True)\n",
    "            \n",
    "            else:\n",
    "                print(f\"{isbn} is not in the DataFrame, checking Google Books API instead\")\n",
    "                # Get book information from Google Books API\n",
    "                title, authors, genre, description = get_book_info(isbn)\n",
    "                # Create a dictionary with book information\n",
    "                book_dict = {'ISBN': isbn, \n",
    "                        'title': title, \n",
    "                        'author': authors, \n",
    "                        'generes': genre, \n",
    "                        'description': description}\n",
    "                # Create a DataFrame with the book information\n",
    "                book_df = pd.DataFrame([book_dict])\n",
    "                #print(book_df)\n",
    "                user_books=pd.concat([user_books, book_df], ignore_index=True)\n",
    "                user_books = user_books.dropna(subset=['title'])\n",
    "                user_books = user_books[user_books['title'] != ''] # get rid of all books that fail to retrieve data from Google API\n",
    "        print(user_books)\n",
    "        return user_books\n",
    "    else:\n",
    "        user_books = pd.DataFrame()\n",
    "        print(\"No input. Skip to next step\")\n",
    "        return user_books #return an empty dataframe if user inputs nothing\n",
    "        \n",
    "        \n",
    "#get_user_books()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by year\n",
    "Prompt user to input a range of years\n",
    "\n",
    "Get books within those years\n",
    "\n",
    "Return *year_books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt user for year range\n",
    "def year_filter (df: pd.DataFrame):\n",
    "    \n",
    "    while True:\n",
    "        year_range = input(\"Enter year range in 'xxxx-yyyy' format (leave blank to include all available years): \")\n",
    "        \n",
    "        if year_range.strip() == '':\n",
    "            print(\"No input. All books will be used.\")\n",
    "            \n",
    "            break\n",
    "            \n",
    "        elif len(year_range.split('-')) != 2:\n",
    "            print(\"Invalid format! Please enter year range in 'xxxx-yyyy' format.\")\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                start_year, end_year = year_range.split('-')\n",
    "                start_year = int(start_year)\n",
    "                end_year = int(end_year)\n",
    "                \n",
    "                if start_year > end_year:\n",
    "                    print(\"Invalid range! Please enter a valid year range.\")\n",
    "                \n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            except ValueError:\n",
    "                print(\"Invalid format! Please enter year range in 'xxxx-yyyy' format.\")\n",
    "\n",
    "    # select rows with matching year range\n",
    "    if year_range.strip() == '':\n",
    "        year_books = df\n",
    "    \n",
    "    else:\n",
    "        year_books = df.loc[(df['year'] >= start_year) & (df['year'] <= end_year)]\n",
    "        \n",
    "        if len(year_books) < 1:\n",
    "            print(\"There are no books available in the specified range! The whole dataset will be used.\")\n",
    "            year_books = df\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    # print results\n",
    "    year_books.to_excel('yearbooks.xlsx', index=False) \n",
    "    \n",
    "    print(f\"{len(year_books['title'].unique())} are remaining in the filtered dataset. \\n\")\n",
    "    \n",
    "    return year_books\n",
    "\n",
    "#year_filter(books_1299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by book length\n",
    "Prompt user to input their reading speed and time available to read\n",
    "\n",
    "Calculate book length and filter accordingly (reading speed*time=>page count)\n",
    "\n",
    "Take data from *year_books* and return *timed_books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt user for reading speed\n",
    "    \n",
    "def speed_filter (df: pd.DataFrame): \n",
    "    \n",
    "    while True:\n",
    "        reading_speed = input(\"Enter your estimated reading speed (pages per hour) (leave blank to set default = 40, which is the human average): \")\n",
    "        \n",
    "        if reading_speed.strip() == '':\n",
    "            reading_speed = 40\n",
    "            print(\"No input. Skip to next step\")\n",
    "            \n",
    "            break\n",
    "    \n",
    "        else:\n",
    "            try:\n",
    "                reading_speed = float(reading_speed)\n",
    "            \n",
    "                break\n",
    "        \n",
    "            except ValueError:\n",
    "                print(\"Invalid input! Please enter a number.\")\n",
    "    \n",
    "\n",
    "    # Prompt user for reading time\n",
    "    while True:\n",
    "        reading_time = input(\"Enter how many hours you can spend reading the book (leave blank if you have all the time in the world): \")\n",
    "        \n",
    "        if reading_time.strip() == '':\n",
    "            reading_time = float('inf')\n",
    "            \n",
    "            break\n",
    "    \n",
    "        else:\n",
    "            try:\n",
    "                reading_time = float(reading_time)\n",
    "                \n",
    "                break\n",
    "        \n",
    "            except ValueError:\n",
    "                print(\"Invalid input! Please enter a number.\")\n",
    "\n",
    "    # Filter the DataFrame based on page count and reading speed/time\n",
    "    timed_books = df.loc[(df['page_count'] / reading_speed) <= reading_time]\n",
    "    timed_books.to_excel('timedbooks.xlsx', index=False) \n",
    "\n",
    "    if len(timed_books) < 1:\n",
    "            print(\"There are no books available in the specified range! The whole dataset will be used.\")\n",
    "            timed_books = df\n",
    "                      \n",
    "    print(f\"{len(timed_books['title'].unique())} are remaining in the filtered dataset. \\n\")\n",
    "\n",
    "    return timed_books\n",
    "\n",
    "#speed_filter(year_filter(books_1299))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Genre\n",
    "\n",
    "Prompt user to select their favourite Genres (see *build_a_recommender.ipynb* for reference)\n",
    "\n",
    "Take data from *timed_books* and return *genre_books* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def genre_filter (df: pd.DataFrame):\n",
    "    \n",
    "    while True:\n",
    "        user_genre=input(f\"Enter you favorite genres, separated by commas (leave blank to skip). \\n \\n Please choose from this available set:\\n \\n {set(df['single_genre'])}\").strip()\n",
    "\n",
    "        if user_genre == \"\":\n",
    "            print(\"No input. Skip to next step\")\n",
    "            genre_books=df\n",
    "            \n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                # Split the input string into a list of genres\n",
    "                user_genre = [genre.strip() for genre in user_genre.split(',')]\n",
    "                # Check if any of the genres in user_genre are included in the single_genre column of timed_books dataframe\n",
    "                genre_books = df[df['single_genre'].isin(user_genre)]\n",
    "                print(f\"You have chosen books from the genre(s): {genre_books['single_genre'].unique()}.\")\n",
    "                \n",
    "                break\n",
    "        \n",
    "            except ValueError:\n",
    "                print(\"Invalid input! Please choose from the available list.\")\n",
    "    \n",
    "    genre_books.to_excel(\"genrebooks.xlsx\")\n",
    "    \n",
    "    if len(genre_books) < 1:\n",
    "            print(\"There are no books available in the specified range! The whole dataset will be used.\")\n",
    "            genre_books = df\n",
    "                      \n",
    "    print(f\"{len(genre_books['title'].unique())} are remaining in the filtered dataset. \\n\")\n",
    "\n",
    "    return genre_books\n",
    "\n",
    "#genre_filter(speed_filter(year_filter(books_1299)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Ratings/Popularity\n",
    "\n",
    "(Final filter?)\n",
    "\n",
    "Prompt user to input a minimum rating & minimum voters count.\n",
    "\n",
    "Take data from *genre_books* and return *filtered_books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rating_filter (df: pd.DataFrame):  \n",
    "    # calculate average rating and voters for genre_books\n",
    "    avg_rating = round(df['rating'].mean(),2)\n",
    "    avg_voters = round(df['voters'].mean(),2)\n",
    "    max_voters = df['voters'].max()\n",
    "\n",
    "    while True:\n",
    "        min_vote=input(f\"Enter the minimum voters count that you want to filter (leave blank to skip). \\n The current average voter count in the dataset is {avg_voters} and the maximum is {max_voters}. \").strip()\n",
    "        \n",
    "        if min_vote.strip() == '':\n",
    "            min_vote = 0\n",
    "            print(\"No input. Skip to next step\")\n",
    "            \n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                min_vote=float(min_vote)\n",
    "                \n",
    "                if min_vote > max_voters or 0>min_vote:\n",
    "                    print(f\"Invalid input! Please choose a number between 0 and {max_voters}.\")\n",
    "                \n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            except ValueError:\n",
    "                print(\"Invalid input! Please enter a number.\")\n",
    "\n",
    "\n",
    "    while True:\n",
    "        min_rating=input(f\"Enter the minimum rating that you want to filter (leave blank to skip). \\n The current average rating in the dataset is {avg_rating} and the maximum rating possible is 5.0. \").strip()\n",
    "        \n",
    "        if min_rating.strip() == '':\n",
    "            min_rating = 0\n",
    "            print(\"No input. Skip to next step\")\n",
    "            \n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                min_rating=float(min_rating)\n",
    "                \n",
    "                if min_rating > 5 or 0> min_rating:\n",
    "                    print(f\"Invalid input! Please choose a number between 0.0 and 5.0.\")\n",
    "                \n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            except ValueError:\n",
    "                print(\"Invalid input! Please enter a number.\")\n",
    "\n",
    "    print(f'You have chosen books with minimum vote count of {min_vote} and minimum ratings of {min_rating}.')\n",
    "\n",
    "    filtered_books = df[(df['rating'] >= min_rating) & (df['voters'] >= min_vote)]\n",
    "    filtered_books.drop_duplicates(subset=['title', 'ISBN'], inplace=True)\n",
    "\n",
    "    print(f\"{len(filtered_books['title'].unique())} are remaining in the filtered dataset. \\n\")\n",
    "    \n",
    "    return filtered_books\n",
    "\n",
    "#rating_filter(genre_filter(speed_filter(year_filter(books_1299))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based filtering\n",
    "\n",
    "Take user input books (done above) *user_books* dataframe and find similar books from the library (tbc)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter year range in 'xxxx-yyyy' format (leave blank to include all available years):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No input. All books will be used.\n",
      "246 are remaining in the filtered dataset. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your estimated reading speed (pages per hour) (leave blank to set default = 40, which is the human average):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No input. Skip to next step\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter how many hours you can spend reading the book (leave blank if you have all the time in the world):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 are remaining in the filtered dataset. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter you favorite genres, separated by commas (leave blank to skip). \n",
      " \n",
      " Please choose from this available set:\n",
      " \n",
      " {'magic', 'gay', 'activities', 'movements', 'neuropsychology', 'leadership', 'inspiration &amp', 'collections &amp', 'comic strips &amp', 'meetings &amp', 'american', 'internal medicine', 'literary criticism', 'reference', 'time management', 'comics', 'superheroes', 'training', 'careers', 'bombay (india)', 'management science', 'personnel management', 'rich &amp', 'graphic novels', 'new business enterprises', 'inspirational', 'comics &amp', 'conflict resolution &amp', 'advertising &amp', 'stocks', 'alternative history', 'drug couriers', 'climate change', 'satire', 'biography', 'self-confidence', 'social skills', 'spirit', 'health &amp', 'art', 'new thought', 'wonders', 'motivational', 'emotions', 'dishes', 'sports &amp', 'global warming &amp', 'international mystery &amp', 'genres', 'communication &amp', 'personal memoirs', 'business', 'relationships', 'corporate finance', 'literary collections', 'organizational behavior', 'administration', 'skills', 'cognitive psychology &amp', 'fiction', 'development', 'social media', 'cooking', 'presentations', 'psychoanalysis', 'religion', 'protection', 'adventure', 'sociology', 'investing', 'web', 'literary', 'historical', 'popular culture', 'small town &amp', 'epic', 'mythical creatures', 'software development &amp', 'citizenship', 'holiday', 'surveys', 'humor', 'war &amp', 'data processing', 'dragons &amp', 'drama', 'environmental conservation &amp', 'general', 'nonfiction', 'science', 'gender studies', 'economics', 'cookies', 'technology &amp', 'traditional', 'music', 'video game art', 'detective', 'personal success', 'autobiography', 'human resources &amp', 'decision-making &amp', 'money management', 'legends &amp', 'problem solving', 'management', '20th century', 'methods', 'assassination', 'crime', 'genres &amp', 'electronic', 'true crime', 'coming of age', 'professional', 'family', 'industries', 'mind &amp', 'political ideologies', 'public finance', 'strategy', 'business communication', 'body', 'mental health', 'personal finance', 'poetry', 'selling', 'action & adventure', 'environmentalists &amp', 'computers', 'desktop applications', 'comics & graphic novels', 'reformation', 'naturalists', 'games &amp', 'happiness', 'customer relations', 'medical', 'theater', 'medical (incl. patients)', 'women', 'performing arts', 'fairy tales', 'folk tales', 'life stages', 'accounting', 'managerial', 'anthologies', 'physiological psychology', 'essays', 'private investigators', 'celebrity &amp', 'musicals', 'police procedural', 'corporate &amp', 'marketing', 'multilevel', 'recreation', 'organizational development', 'business history', 'fantasy fiction', 'business & economics', 'women sleuths', 'motivational &amp', 'securities', 'business development', 'horror', 'dystopian', 'psychology', 'domestic', 'styles', 'courses &amp', 'family life', 'political', 'e-commerce', 'self-esteem', 'self-help', 'self-management', 'success', 'suspense', 'online trading', 'entertainment &amp', 'media tie-in', 'cognition', 'business &amp', 'social psychology', 'information management', 'consumer behavior', 'retrieval', 'family &amp', 'shakespeare', 'humorous', 'baking', 'science fiction', 'paranormal', 'investments &amp', 'urban', 'entertainment', 'boston (mass.)', 'mythology', 'motivation (psychology)', 'science &amp', 'rural', 'promotion', 'workplace culture', 'political science', 'business mathematics', 'mystery & detective', 'organized crime', 'space opera', 'lgbt', 'healthy living', 'juvenile fiction', 'fitness', 'noir', 'entrepreneurship', 'military', 'supernatural', 'health', 'video &amp', 'curiosities &amp', 'mediation', 'technological', 'action &amp', 'young adult fiction', 'stress management', 'history &amp', 'legal', 'mentoring & coaching', 'modern', 'cartoons', 'television', 'fantasy &amp', 'fantasy', 'civics &amp', 'personal growth', 'technology', 'fairy tales &amp', 'feminist theory', 'feminism &amp', 'finance', 'brain', 'engineering', 'topic', 'espionage', 'information technology', 'animals', 'contemporary', 'interpersonal relations', 'budgeting', 'form', 'public affairs &amp', 'social activists', 'dark fantasy', 'bonheur', 'philosophy', 'thrillers', 'physicians', 'storage &amp', 'literary figures', 'nature', 'cozy', 'creative ability', 'social science', 'computers &amp', 'strategic planning', 'system administration', 'folklore', 'manga', 'financial', 'theory', 'electronic commerce', 'mind', 'nan', 'short stories (single author)', 'criminology', 'hard-boiled', 'biography &amp', 'humorous stories', 'amateur sleuth', 'comedy', 'social themes', 'terrorism', 'classics', 'mystery &amp', 'juvenile nonfiction', 'friendship', 'internet marketing', 'alien contact', 'sales &amp', 'health care delivery', 'history', 'letters', 'psychological', 'school age', 'sports', 'famous', 'soccer'} \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No input. Skip to next step\n",
      "246 are remaining in the filtered dataset. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the minimum voters count that you want to filter (leave blank to skip). \n",
      " The current average voter count in the dataset is 780.89 and the maximum is 38526.0.  40\n",
      "Enter the minimum rating that you want to filter (leave blank to skip). \n",
      " The current average rating in the dataset is 4.41 and the maximum rating possible is 5.0.  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have chosen books with minimum vote count of 40.0 and minimum ratings of 4.0.\n",
      "141 are remaining in the filtered dataset. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mucki\\AppData\\Local\\Temp\\ipykernel_4836\\2916563890.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_books.drop_duplicates(subset=['title', 'ISBN'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize data\n",
    "\n",
    "data = rating_filter(genre_filter(speed_filter(year_filter(books_1299)))) # This line sequentially runs through all filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a list of your favourite books as ISBNs separated by commas (leave blank if you don't have a favourite):  9780062316110, 9780393338102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following ISBNs are valid:  ['9780062316110', '9780393338102']\n",
      "9780062316110 is not in the DataFrame, checking Google Books API instead\n",
      "Error: missing book information.\n",
      "9780393338102 is not in the DataFrame, checking Google Books API instead\n",
      "Error: missing book information.\n",
      "Empty DataFrame\n",
      "Columns: [ISBN, title, author, generes, description]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "user_books = get_user_books()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for calculating cosine similarity\n",
    "\n",
    "def cos_sim_get (descriptions_clean = [], i = 0):\n",
    "    if len(descriptions_clean) != 0:\n",
    "        try:\n",
    "            # Create a fit_transf vector\n",
    "            try:\n",
    "                desc_vect = TfidfVectorizer(ngram_range=(1, 3) , max_df = 0.85, min_df = 0.1) # This throws an error if pruning removes to many items\n",
    "                desc_vectF = desc_vect.fit_transform(descriptions_clean)                      # try: tfidfVect, except Error: countVect\n",
    "                \n",
    "            except ValueError:                                                                # If the tfidf vectorizer fails, a count vectorizer may work\n",
    "                desc_vect =  CountVectorizer(ngram_range=(1,3))\n",
    "                desc_vectF = desc_vect.fit_transform(descriptions_clean)                       \n",
    "            \n",
    "            # Calculate cos_sim \n",
    "\n",
    "            c_s = cosine_similarity(desc_vectF)\n",
    "\n",
    "            sim_scores = list(enumerate(c_s[i])) # The index i allows to access the respective row for a book: default is 0, i.e. the first row\n",
    "            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True) # Sort descending on cos_sim values\n",
    "            sim_scores = sim_scores[1:]\n",
    "            \n",
    "            return sim_scores\n",
    "        \n",
    "        except ValueError: \n",
    "            sim_scores = []\n",
    "            print(f\"Unable to vectorize {descriptions_clean}!\")\n",
    "            \n",
    "            return sim_scores\n",
    "\n",
    "    else:\n",
    "        sim_scores = []\n",
    "        print(f\"Unable to vectorize {descriptions_clean}!\")\n",
    "\n",
    "        \n",
    "        return sim_scores\n",
    "\n",
    "\n",
    "#cos_sim_get(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preparing the list for cos_sim analysis\n",
    "\n",
    "def desc_get (df: pd.DataFrame):\n",
    "\n",
    "    if \"description\" in df.columns and df.empty==False:\n",
    "        df[\"description\"].dropna() # drop missing values\n",
    "        df.drop_duplicates(subset=['title', 'description'], inplace=True)\n",
    "\n",
    "        desc_list = df[\"description\"].astype(str).tolist()\n",
    "\n",
    "        mystopwords = set(stopwords.words('english')) # use default NLTK stopword list;\n",
    "\n",
    "        descriptions_clean = [text.lower() for text in desc_list]\n",
    "        descriptions_clean = [\" \".join(text.split()) for text in descriptions_clean]  #remove dubble spaces\n",
    "        descriptions_clean = [\"\".join([l for l in text if l not in punctuation]) for text in descriptions_clean] #remove punctuation\n",
    "        descriptions_clean = [\" \".join(word for word in text.split() if word not in mystopwords) for text in descriptions_clean] #remove stopwrds\n",
    "        \n",
    "        return descriptions_clean\n",
    "    \n",
    "    else:\n",
    "        print(f\"There are no descriptions to analyze in {df}!\") \n",
    "        descriptions_clean=[]\n",
    "        \n",
    "        return descriptions_clean\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cos_sim_get(desc_get(user_books)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is just for convenience\\n\\ncos_sim_user = cos_sim_get(desc_get(user_books)) #apply cos_sim to the list of user input ISBNs\\ncos_sim_filter = cos_sim_get(desc_get(data))\\n\\nprint(f\"{cos_sim_user}\\n\\n{cos_sim_filter}\")'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' This is just for convenience\n",
    "\n",
    "cos_sim_user = cos_sim_get(desc_get(user_books)) #apply cos_sim to the list of user input ISBNs\n",
    "cos_sim_filter = cos_sim_get(desc_get(data))\n",
    "\n",
    "print(f\"{cos_sim_user}\\n\\n{cos_sim_filter}\")'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to tweak the square brackets ([-1][1] and [0][1]) in the if loops to resolve the \"index out of range\" errors. But im not sure it is working correct anymore. The threshold is set quite high at 0.8 and yet it still works. Help me check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cos_sim_between (desc_1 = [], desc_2 = []):\n",
    "    unique_index = [] # This will help get rid of duplicates\n",
    "   \n",
    "    if len(desc_1)!=0 and len(desc_2)!=0:\n",
    "        index_books = [] # Thats the return, a list of book indices\n",
    "        \n",
    "        # Check how good the cos_sim within user_books is; set to the lowest cos_sim in filter_books or 0.1\n",
    "        \n",
    "        if cos_sim_get(desc_2)[9][1] > 0.05:  # \"[9][1]\" refers to the 10th highest cos_sim within the filtered books\n",
    "            tresh = cos_sim_get(desc_2)[9][1] # this is used as a treshold assuming that the descriptions are similar to the same degree\n",
    "            \n",
    "        else:\n",
    "            tresh = 0.8 # This is an arbitrary treshold, lemme know what you think\n",
    "            \n",
    "        print(f\"The treshold cos_sim is {tresh}\") # Check threshold level\n",
    "        \n",
    "        if cos_sim_get(desc_1)[0][1] > tresh: ## If the largest cos_sim within user_books is smaller than treshold, collaps all descriptions into one to proceed\n",
    "            #print('Top-Route')     \n",
    "            \n",
    "            desc_1 = [\"\".join(desc_1)]        # This assumes that user_books are similar enough to be considered the same content/topic and can be collapsed int one description\n",
    "            \n",
    "            descriptions = desc_1             # Adding filter_books UNDER user_books for easy access later\n",
    "            \n",
    "            for i in desc_2:\n",
    "                descriptions.append(i)\n",
    "            #print(descriptions[:2], \"\\n\\n\\n\")\n",
    "            \n",
    "            cos_sim_tresh = cos_sim_get(descriptions)\n",
    "            index_books = [i[0] for i in cos_sim_tresh] # These are the books most similar to the collapsed description of user_books\n",
    "            \n",
    "            #print(f\"Top 2 cos_sim: {cos_sim_get(desc_1)[1][2]}, {cos_sim_get(desc_1)[1][1]}\")\n",
    "            \n",
    "            for i in index_books:\n",
    "                if i not in unique_index:\n",
    "                    unique_index.append(i)\n",
    "                    \n",
    "            return index_books\n",
    "        \n",
    "        else:                                 ## Else, the programm will retrieve the two books with the largest cos_sim for each in user_books, up to the number specified by the user\n",
    "            #print('Bottom-Route')\n",
    "            \n",
    "            descriptions = desc_1\n",
    "            for i in desc_2:\n",
    "                descriptions.append(i)\n",
    "                \n",
    "            try:\n",
    "                i = 0\n",
    "                while i < len(descriptions):   \n",
    "                    top_2 = []\n",
    "                    j = 0\n",
    "                    \n",
    "                    while j < 2:              \n",
    "                        top_2.append(cos_sim_get(descriptions, i)[j + len(user_books)][0]) # the term \"j + len(user_books)\" makes sure we access the cos_sim of the current userbook with the respective filtered_books\n",
    "                        j+=1\n",
    "                    \n",
    "                    for t in top_2: # Append each top two pair to the index in order\n",
    "                        index_books.append(t)\n",
    "                \n",
    "                    i+=1\n",
    "            \n",
    "            except ValueError:\n",
    "                print(\"Invalid input! Please enter a number.\")\n",
    "            \n",
    "            for i in index_books:\n",
    "                if i not in unique_index: # drop duplicates\n",
    "                    unique_index.append(i)\n",
    "                    \n",
    "            return unique_index\n",
    "    \n",
    "    else:\n",
    "        print('Error')\n",
    "        \n",
    "        return unique_index\n",
    "\n",
    "#cos_sim_between(desc_get(user_books), desc_get(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_1299.sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_same_author (df: pd.DataFrame):  \n",
    "    if df.empty==False:\n",
    "        try:\n",
    "            # Extract unique authors and publishers from user_books dataframe\n",
    "            unique_authors = df['author'].unique()\n",
    "            \n",
    "            # Filter books_1299 dataframe based on matching author or publisher\n",
    "            check_author = books_1299[(books_1299['author'].isin(unique_authors))]\n",
    "            same_author = check_author.merge(user_books[['ISBN', 'author']], on='ISBN', how='left')\n",
    "            same_author.drop_duplicates(subset=['title', 'ISBN'], inplace=True)\n",
    "            same_author = same_author.filter(['title', 'author'])\n",
    "            if len(same_author)==0:\n",
    "                print(\"We found no books from the same author(s) of your favourite books in our database.\")\n",
    "            else:    \n",
    "                print(\"You might also be interested in these books that came from the same author(s) as your favourite books:\")\n",
    "\n",
    "            return same_author\n",
    "        except KeyError:\n",
    "            same_author = pd.DataFrame()\n",
    "            return same_author\n",
    "    else:\n",
    "        same_author = pd.DataFrame()\n",
    "        print(\"No input. Skip to next step\")\n",
    "        return same_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the respective titles\n",
    "\n",
    "def recommended_books(user_books: pd.DataFrame, filtered_books: pd.DataFrame):\n",
    "    if len(desc_get(user_books)) !=0 and len(desc_get(filtered_books))!=0: #if user input no favourite isbns earlier, or has too strict criterias, no content-based analysis can be done, so quit\n",
    "        try:    \n",
    "            rec_count = input(\"How many book recommendations would you like? \")\n",
    "            if rec_count.strip() == '' or rec_count==0: #if user inputs no or 0, quit \n",
    "                recommendations = pd.DataFrame()\n",
    "                print(\"No content-based recommendation.\")\n",
    "                return recommendations\n",
    "            \n",
    "            book_ind = cos_sim_between(desc_get(user_books), desc_get(filtered_books))  # A list of book indices\n",
    "            book_titles = []\n",
    "\n",
    "            #print(\"book_ind:\", book_ind)\n",
    "            #print(\"filtered_books index:\", filtered_books.index)\n",
    "\n",
    "            for ind in book_ind:  # Loop over all available indices\n",
    "                if ind in filtered_books.index:  # check whether the index is part of the filtered books\n",
    "                    book_titles.append(filtered_books.iloc[ind]['title'])\n",
    "                #else:\n",
    "                    #print(\"Index not found:\", ind)\n",
    "                    \n",
    "            #print(\"book_titles:\", book_titles)\n",
    "            \n",
    "            \n",
    "            if len(book_titles) < int(rec_count): # Get the list of book titles to the right length\n",
    "                \n",
    "                if len(book_titles) == 1: \n",
    "                    print(f\"Based on the filters, only 1 available title may interest you: \\n\\n\")\n",
    "                    recommendations = pd.DataFrame(book_titles, columns = [\"title\"])\n",
    "                    \n",
    "                elif len(book_titles) > 1:\n",
    "                    print(f\"Based on the filters, only {len(book_titles)} available titles may interest you: \\n\\n\")\n",
    "                    recommendations = pd.DataFrame(book_titles, columns = [\"title\"])\n",
    "                    \n",
    "                else:\n",
    "                    y_n = input(f\"Based on the filters, none of the titles in the dataset fit your interests.\\n How abut other books from the same authors? (y/n)\\n\").strip().lower()\n",
    "                                # Here we ask the user if they want another kind of recommendations, i.e. other books from the same authors\n",
    "                    \n",
    "                    while True: # This whole section forces a y/n response and returns get_same_author(user_books) for yes\n",
    "                        while len(y_n) != 1: \n",
    "                            y_n = input(f\"Please enter your preference as y for 'yes' or n for 'no'.\")\n",
    "                    \n",
    "                        try:\n",
    "                            if y_n == \"n\":\n",
    "                                print(f\"Unfortunately there are no recommendations possible after filtering. \\n\\n\")\n",
    "                                break\n",
    "                        \n",
    "                            elif y_n == \"y\":\n",
    "                                if len(get_same_author(user_books)) > 0:\n",
    "                                    recommendations = get_same_author(user_books)\n",
    "                            \n",
    "                                    return recommendations\n",
    "                                    break\n",
    "                                \n",
    "                                else:\n",
    "                                    break\n",
    "                        \n",
    "                            else:\n",
    "                                y_n = input(f\"Please enter your preference as y for 'yes' or n for 'no'.\")\n",
    "\n",
    "                        except ValueError:\n",
    "                                print(f\"Please enter your preference as y for 'yes' or n for 'no'.\")\n",
    "            \n",
    "            \n",
    "            elif len(book_titles) == int(rec_count):\n",
    "                recommendations = pd.DataFrame(book_titles, columns = [\"title\"])\n",
    "\n",
    "                return recommendations\n",
    "\n",
    "            else: \n",
    "                while len(book_titles) > int(rec_count): # Keep removing the last item until the list has the correct length\n",
    "                    book_titles.pop()\n",
    "                \n",
    "                recommendations = pd.DataFrame(book_titles, columns = [\"title\"])\n",
    "                print(f\"Here are {rec_count} books in our database that best fit your interest: \")\n",
    "                return recommendations\n",
    "            \n",
    "\n",
    "        except ValueError: #here for testing purposes, remember to change back to valueerror\n",
    "                print(\"Invalid input! Please enter a number.\")\n",
    "    else:\n",
    "        recommendations = pd.DataFrame()\n",
    "        print(\"We could not find a content-based recommendation as you did not input any ISBNs, or the books you have entered are not available in our database and Google Books API.\")\n",
    "        return recommendations\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results\n",
    "\n",
    "\n",
    "\n",
    "- Books that are filtered through user preference (by year, length, genre, popularity/ratings)\n",
    "\n",
    "- Books that match with their list of favourite books by content\n",
    "\n",
    "Result: User-specified top N filtered_books most similar (based on the descriptions) to user_books as a dataframe\n",
    "\n",
    "\n",
    "- Aggregate all into one final excel file as final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are all the books in our database that match your preferences:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>rating</th>\n",
       "      <th>voters</th>\n",
       "      <th>description</th>\n",
       "      <th>page_count</th>\n",
       "      <th>generes</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>published_date</th>\n",
       "      <th>year</th>\n",
       "      <th>single_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Attack on Titan: Volume 13</td>\n",
       "      <td>Hajime Isayama</td>\n",
       "      <td>4.6</td>\n",
       "      <td>428.0</td>\n",
       "      <td>NO SAFE PLACE LEFT At great cost to the Garris...</td>\n",
       "      <td>192</td>\n",
       "      <td>comics,graphic novels</td>\n",
       "      <td>9781612626864</td>\n",
       "      <td>2014-07-31</td>\n",
       "      <td>2014</td>\n",
       "      <td>comics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Painted Man (The Demon Cycle, Book 1)</td>\n",
       "      <td>Peter V. Brett</td>\n",
       "      <td>4.5</td>\n",
       "      <td>577.0</td>\n",
       "      <td>The stunning debut fantasy novel from author P...</td>\n",
       "      <td>544</td>\n",
       "      <td>fiction,fantasy,dark fantasy</td>\n",
       "      <td>9780007287758</td>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>2009</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>A Feast for Crows (A Song of Ice and Fire, Boo...</td>\n",
       "      <td>George R.R. Martin</td>\n",
       "      <td>4.5</td>\n",
       "      <td>832.0</td>\n",
       "      <td>HBO’s hit series A GAME OF THRONES is based on...</td>\n",
       "      <td>864</td>\n",
       "      <td>fiction</td>\n",
       "      <td>9780007369218</td>\n",
       "      <td>2011-02-24</td>\n",
       "      <td>2011</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>God of War: The Official Novelization</td>\n",
       "      <td>J.M. Barlog</td>\n",
       "      <td>4.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>The novelization of the highly anticipated God...</td>\n",
       "      <td>400</td>\n",
       "      <td>fiction,media tie-in</td>\n",
       "      <td>9781789090154</td>\n",
       "      <td>2018-08-28</td>\n",
       "      <td>2018</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Edgedancer: From the Stormlight Archive</td>\n",
       "      <td>Brandon Sanderson</td>\n",
       "      <td>4.8</td>\n",
       "      <td>221.0</td>\n",
       "      <td>From #1 New York Times bestselling author Bran...</td>\n",
       "      <td>226</td>\n",
       "      <td>fiction,fantasy,epic</td>\n",
       "      <td>9781250166609</td>\n",
       "      <td>2017-10-17</td>\n",
       "      <td>2017</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title   \n",
       "0           0                         Attack on Titan: Volume 13  \\\n",
       "4           4          The Painted Man (The Demon Cycle, Book 1)   \n",
       "5           5  A Feast for Crows (A Song of Ice and Fire, Boo...   \n",
       "6           6              God of War: The Official Novelization   \n",
       "7           7            Edgedancer: From the Stormlight Archive   \n",
       "\n",
       "               author  rating  voters   \n",
       "0      Hajime Isayama     4.6   428.0  \\\n",
       "4      Peter V. Brett     4.5   577.0   \n",
       "5  George R.R. Martin     4.5   832.0   \n",
       "6         J.M. Barlog     4.5    94.0   \n",
       "7   Brandon Sanderson     4.8   221.0   \n",
       "\n",
       "                                         description  page_count   \n",
       "0  NO SAFE PLACE LEFT At great cost to the Garris...         192  \\\n",
       "4  The stunning debut fantasy novel from author P...         544   \n",
       "5  HBO’s hit series A GAME OF THRONES is based on...         864   \n",
       "6  The novelization of the highly anticipated God...         400   \n",
       "7  From #1 New York Times bestselling author Bran...         226   \n",
       "\n",
       "                        generes           ISBN published_date  year   \n",
       "0         comics,graphic novels  9781612626864     2014-07-31  2014  \\\n",
       "4  fiction,fantasy,dark fantasy  9780007287758     2009-01-08  2009   \n",
       "5                       fiction  9780007369218     2011-02-24  2011   \n",
       "6          fiction,media tie-in  9781789090154     2018-08-28  2018   \n",
       "7          fiction,fantasy,epic  9781250166609     2017-10-17  2017   \n",
       "\n",
       "  single_genre  \n",
       "0       comics  \n",
       "4      fiction  \n",
       "5      fiction  \n",
       "6      fiction  \n",
       "7      fiction  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if data.empty == True: ## Use recommendations insetead of filtered_books?\n",
    "    print(\"No results based on your filters. Please try again with different filter preferences.\")\n",
    "elif len(data)==len(books_1299): #if no filtering was done at all\n",
    "    print(\"No input found.\")\n",
    "else:    \n",
    "    print(\"Here are all the books in our database that match your preferences:\")\n",
    "    data.to_excel(\"filteredbooks.xlsx\")\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No input. Skip to next step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_same_author(user_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no descriptions to analyze in Empty DataFrame\n",
      "Columns: [ISBN, title, author, generes, description]\n",
      "Index: []!\n",
      "We could not find a content-based recommendation as you did not input any ISBNs, or the books you have entered are not available in our database and Google Books API.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add results of content based filtering\n",
    "recommended_books(user_books, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9781640290112,9780008150907"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
